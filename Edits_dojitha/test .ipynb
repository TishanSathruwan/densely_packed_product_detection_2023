{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõçÔ∏è SKU110K Dataset\n",
    "\n",
    "Here you will find the images and annotations (labels) for the SKU110K dataset. Also included is a notebook containing the process to download the SKU110K dataset and convert the annotations to a variety of formats (e.g., YOLOv5).\n",
    "\n",
    "## Files\n",
    "\n",
    "This dataset contains the labels comprising the bounding boxes (only one class for all detections), as well as all images for the SKU110K dataset. The included notebook is meant to be run locally, and the code shows how to use the Kaggle CLI for downloading the images to their corresponding location.\n",
    "\n",
    "In order to use YOLOv5, the included notebook shows how to save the annotations in the correct format:\n",
    "- One row per object\n",
    "- Each row is `class` &nbsp; `x_center` &nbsp; `y_center` &nbsp; `width` &nbsp; `height` format\n",
    "- Box coordinates must be in **normalized xywh** format (from 0 - 1). If your boxes are in pixels:\n",
    "    - Divide `x_center` and `width` by image width\n",
    "    - Divive `y_center` and `height` by image height\n",
    "- Class numbers are zero-indexed (start from 0)\n",
    "\n",
    "Although the included annotations use my preferred directory structure for organization (as detailed below), they can all be contained in a single file as well.\n",
    "\n",
    "    .\n",
    "    ‚îú‚îÄ‚îÄ convert_yolov5.ipynb\n",
    "    ‚îú‚îÄ‚îÄ data_kaggle.yaml\n",
    "    ‚îú‚îÄ‚îÄ README.md\n",
    "    ‚îú‚îÄ‚îÄ SKU110K_fixed    \n",
    "        ‚îú‚îÄ‚îÄ images\n",
    "            ‚îú‚îÄ‚îÄ test\n",
    "            ‚îú‚îÄ‚îÄ train\n",
    "            ‚îú‚îÄ‚îÄ val\n",
    "        ‚îú‚îÄ‚îÄ labels\n",
    "            ‚îú‚îÄ‚îÄ test\n",
    "            ‚îú‚îÄ‚îÄ train\n",
    "            ‚îú‚îÄ‚îÄ val\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I'd love to acknowledge the original authors: https://github.com/eg4000/SKU110K_CVPR19\n",
    "\n",
    "---\n",
    "\n",
    "üêû If you find any bugs or have any questions regarding these notebooks, please open an issue. I'll address it as soon as I can. \n",
    "\n",
    "üê¶ Reach out on [Twitter](https://twitter.com/datasith) if you have any questions. \n",
    "\n",
    "üîó Please cite the original authors if you use this version of the dataset for your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\User Dojitha\\UoM\\ENTC\\Sem 5\\CV\\project Densly packed\\densely_packed_product_detection_2023\\Edits_dojitha\\test .ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m plot_image_with_boxes(test0_path_image, test0_path_labels)\n",
      "\u001b[1;32md:\\User Dojitha\\UoM\\ENTC\\Sem 5\\CV\\project Densly packed\\densely_packed_product_detection_2023\\Edits_dojitha\\test .ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_image_with_boxes\u001b[39m(image_path, text_file_path):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(img, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(text_file_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/User%20Dojitha/UoM/ENTC/Sem%205/CV/project%20Densly%20packed/densely_packed_product_detection_2023/Edits_dojitha/test%20.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "test0_path_labels = \"/kaggle/input/sku110k-annotations/SKU110K_fixed/labels/test/test_0.txt\"\n",
    "test0_path_image = \"/kaggle/input/sku110k/SKU110K_fixed/test/test_0.jpg\"\n",
    "\n",
    "\n",
    "#plot image with bounding boxes\n",
    "# text file format: class x_center y_center width height (all values are relative to image size, i.e. in [0,1])\n",
    "\n",
    "def plot_image_with_boxes(image_path, text_file_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    f = open(text_file_path, \"r\")\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        x_center = float(line[1])\n",
    "        y_center = float(line[2])\n",
    "        width = float(line[3])\n",
    "        height = float(line[4])\n",
    "        x1 = int((x_center - width / 2) * img.shape[1])\n",
    "        y1 = int((y_center - height / 2) * img.shape[0])\n",
    "        x2 = int((x_center + width / 2) * img.shape[1])\n",
    "        y2 = int((y_center + height / 2) * img.shape[0])\n",
    "        img = cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "plot_image_with_boxes(test0_path_image, test0_path_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert yolo format\n",
    "# yolo format: class x_center y_center width height (all values are relative to image size, i.e. in [0,1])\n",
    "# coco format: [x1, y1, width, height] (x1,y1) is top left corner\n",
    "\n",
    "def convert_yolo_to_coco(text_file_path):\n",
    "    f = open(text_file_path, \"r\")\n",
    "    coco_format = []\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        x_center = float(line[1])\n",
    "        y_center = float(line[2])\n",
    "        width = float(line[3])\n",
    "        height = float(line[4])\n",
    "        x1 = int((x_center - width / 2) * img.shape[1])\n",
    "        y1 = int((y_center - height / 2) * img.shape[0])\n",
    "        width = int(width * img.shape[1])\n",
    "        height = int(height * img.shape[0])\n",
    "        coco_format.append([x1, y1, width, height])\n",
    "    return coco_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot image\n",
    "\n",
    "path = '/kaggle/working/test/images/train_740_jpg.rf.e5c6a6f890f7418d6f01058bf3e6c4f1.jpg'\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.5M/21.5M [00:09<00:00, 2.29MB/s]\n",
      "New https://pypi.org/project/ultralytics/8.0.200 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.173  Python-3.11.4 torch-2.0.1+cpu CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=D:\\User Dojitha\\UoM\\ENTC\\Sem 5\\CV\\Project Densly packed\\minidataset\\data.yaml, epochs=1, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\User Dojitha\\UoM\\ENTC\\Sem 5\\CV\\Project Densly packed\\minidataset\\labels\\train... 98 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [00:00<00:00, 334.65it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\User Dojitha\\UoM\\ENTC\\Sem 5\\CV\\Project Densly packed\\minidataset\\labels\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\User Dojitha\\UoM\\ENTC\\Sem 5\\CV\\Project Densly packed\\minidataset\\labels\\val... 21 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:00<00:00, 163.53it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\User Dojitha\\UoM\\ENTC\\Sem 5\\CV\\Project Densly packed\\minidataset\\labels\\val.cache\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/1         0G      2.692      3.457      1.652        308        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [02:04<00:00, 17.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:17<00:00, 17.49s/it]\n",
      "                   all         21       3750      0.513      0.445      0.449      0.222\n",
      "\n",
      "1 epochs completed in 0.042 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.173  Python-3.11.4 torch-2.0.1+cpu CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:08<00:00,  8.93s/it]\n",
      "                   all         21       3750      0.513      0.445      0.449      0.222\n",
      "Speed: 2.3ms preprocess, 293.1ms inference, 0.0ms loss, 23.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000024458287D10>\n",
       "fitness: 0.24499226715613961\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.22229])\n",
       "names: {0: 'object'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.5132940702709552, 'metrics/recall(B)': 0.44533333333333336, 'metrics/mAP50(B)': 0.44926782874720406, 'metrics/mAP50-95(B)': 0.22229498253491023, 'fitness': 0.24499226715613961}\n",
       "save_dir: WindowsPath('runs/detect/train')\n",
       "speed: {'preprocess': 2.2534642900739397, 'inference': 293.09835888090583, 'loss': 0.0, 'postprocess': 23.894446236746653}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model  = YOLO('yolov8s.pt')\n",
    "model.train(data = 'D:\\\\User Dojitha\\\\UoM\\\\ENTC\\\\Sem 5\\\\CV\\\\Project Densly packed\\\\minidataset\\\\data.yaml',epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained yolo model\n",
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "\n",
    "#load the image\n",
    "img_path = \"D:/User Dojitha/UoM/ENTC/Sem 5/CV/Project Densly packed/minidataset/images/val/val_0.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show image with opencv\n",
    "img = cv.imread(img_path)\n",
    "cv.namedWindow('image', cv.WINDOW_NORMAL)\n",
    "cv.imshow('image',img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 26 bottles, 1 refrigerator, 184.4ms\n",
      "Speed: 9.4ms preprocess, 184.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "orig_img: array([[[ 55,  73,  72],\n",
      "        [ 59,  77,  76],\n",
      "        [ 63,  81,  80],\n",
      "        ...,\n",
      "        [ 76,  88,  82],\n",
      "        [ 75,  87,  81],\n",
      "        [ 74,  86,  80]],\n",
      "\n",
      "       [[ 54,  72,  71],\n",
      "        [ 56,  74,  73],\n",
      "        [ 59,  77,  76],\n",
      "        ...,\n",
      "        [ 76,  88,  82],\n",
      "        [ 75,  87,  81],\n",
      "        [ 75,  87,  81]],\n",
      "\n",
      "       [[ 54,  72,  71],\n",
      "        [ 54,  72,  71],\n",
      "        [ 55,  73,  72],\n",
      "        ...,\n",
      "        [ 76,  88,  82],\n",
      "        [ 76,  88,  82],\n",
      "        [ 75,  87,  81]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[135, 141, 146],\n",
      "        [135, 141, 146],\n",
      "        [135, 141, 146],\n",
      "        ...,\n",
      "        [120, 134, 132],\n",
      "        [118, 132, 130],\n",
      "        [122, 136, 134]],\n",
      "\n",
      "       [[133, 139, 144],\n",
      "        [133, 139, 144],\n",
      "        [133, 139, 144],\n",
      "        ...,\n",
      "        [123, 137, 135],\n",
      "        [122, 136, 134],\n",
      "        [125, 139, 137]],\n",
      "\n",
      "       [[131, 137, 142],\n",
      "        [131, 137, 142],\n",
      "        [131, 137, 142],\n",
      "        ...,\n",
      "        [125, 139, 137],\n",
      "        [125, 139, 137],\n",
      "        [128, 142, 140]]], dtype=uint8)\n",
      "orig_shape: (4160, 2336)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 9.380817413330078, 'inference': 184.36646461486816, 'postprocess': 1.997232437133789}]\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(img_path)\n",
    "\n",
    "#predict the bounding boxes\n",
    "results = model(img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
